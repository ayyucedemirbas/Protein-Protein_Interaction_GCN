{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hwZrnP1i0R6Y"
      },
      "outputs": [],
      "source": [
        "import os, urllib.request, zipfile, json\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "prf11-f60Yly"
      },
      "outputs": [],
      "source": [
        "DATA_URL = \"https://snap.stanford.edu/graphsage/ppi.zip\"\n",
        "DATA_DIR = \"ppi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2jrxjBp2tjh",
        "outputId": "f77ddb7a-52f5-4e8e-c555-484c7fed28f3"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "    zip_path, _ = urllib.request.urlretrieve(DATA_URL)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3RdfKXDC0hR0"
      },
      "outputs": [],
      "source": [
        "X = np.load(os.path.join(DATA_DIR, \"ppi/ppi-feats.npy\"))\n",
        "with open(os.path.join(DATA_DIR,\"ppi/ppi-id_map.json\")) as f: id_map = json.load(f)\n",
        "with open(os.path.join(DATA_DIR,\"ppi/ppi-class_map.json\")) as f: class_map = json.load(f)\n",
        "with open(os.path.join(DATA_DIR,\"ppi/ppi-G.json\")) as f: G_json = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yhx0p_580kJt"
      },
      "outputs": [],
      "source": [
        "n_classes = len(next(iter(class_map.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPN5i88N0mUe"
      },
      "outputs": [],
      "source": [
        "Y = np.zeros((X.shape[0], n_classes), dtype=np.int32)\n",
        "train_mask = np.zeros(X.shape[0], dtype=bool)\n",
        "val_mask = np.zeros(X.shape[0], dtype=bool)\n",
        "test_mask = np.zeros(X.shape[0], dtype=bool)\n",
        "\n",
        "for node in G_json['nodes']:\n",
        "    node_id = str(node['id'])\n",
        "    idx = id_map[node_id]\n",
        "    Y[idx] = class_map[node_id]\n",
        "\n",
        "    if node.get('test', False):\n",
        "        test_mask[idx] = True\n",
        "    elif node.get('val', False):\n",
        "        val_mask[idx] = True\n",
        "    else:\n",
        "        train_mask[idx] = True\n",
        "\n",
        "N, F = X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HO4F3c70peS"
      },
      "outputs": [],
      "source": [
        "X = (X - X[train_mask].mean(axis=0)) / (X[train_mask].std(axis=0) + 1e-6)\n",
        "X = np.nan_to_num(X)\n",
        "\n",
        "raw_links = G_json[\"links\"]\n",
        "clean_links = [(l['source'], l['target']) for l in raw_links if l['source'] != l['target']]\n",
        "unique_links = set(clean_links)\n",
        "\n",
        "# Symmetrize\n",
        "rows = [u for u,v in unique_links] + [v for u,v in unique_links]\n",
        "cols = [v for u,v in unique_links] + [u for u,v in unique_links]\n",
        "data = np.ones(len(rows), dtype=np.float32)\n",
        "\n",
        "A = sp.coo_matrix((data, (rows, cols)), shape=(N, N))\n",
        "A.setdiag(1.0)  # Add self-loops\n",
        "A = A.tocsr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4SLqm7DA0uAa"
      },
      "outputs": [],
      "source": [
        "deg = np.array(A.sum(1)).flatten()\n",
        "d_inv_sqrt = np.power(deg, -0.5)\n",
        "d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "A_norm = D_inv_sqrt @ A @ D_inv_sqrt  # A_hat = D^(-1/2) A D^(-1/2)\n",
        "\n",
        "A_norm = A_norm.tocoo()\n",
        "A_sp = tf.SparseTensor(\n",
        "    indices=np.vstack((A_norm.row, A_norm.col)).T,\n",
        "    values=A_norm.data.astype(np.float32),\n",
        "    dense_shape=A_norm.shape\n",
        ")\n",
        "A_sp = tf.sparse.reorder(A_sp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tdS3LSQ0yGF"
      },
      "outputs": [],
      "source": [
        "class GraphConvLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, out_dim, activation=None, use_bias=True,\n",
        "                 dropout_rate=0.0, norm_type='layer', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.out_dim = out_dim\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        F_in = input_shape[0][-1]\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name=\"W\",\n",
        "            shape=(F_in, self.out_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.b = self.add_weight(\n",
        "                name=\"bias\",\n",
        "                shape=(self.out_dim,),\n",
        "                initializer=\"zeros\",\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        if F_in != self.out_dim:\n",
        "            self.W_skip = self.add_weight(\n",
        "                name=\"W_skip\",\n",
        "                shape=(F_in, self.out_dim),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True\n",
        "            )\n",
        "        else:\n",
        "            self.W_skip = None\n",
        "\n",
        "        if self.norm_type == 'batch':\n",
        "            self.norm = BatchNormalization()\n",
        "        elif self.norm_type == 'layer':\n",
        "            self.norm = LayerNormalization()\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "        self.dropout = Dropout(self.dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        X, A_norm = inputs  # X: (N,F), A_norm: sparse (N,N)\n",
        "\n",
        "        # Graph convolution: A_norm @ X @ W\n",
        "        supports = tf.matmul(X, self.W)  # (N, out_dim)\n",
        "        output = tf.sparse.sparse_dense_matmul(A_norm, supports)\n",
        "\n",
        "        \n",
        "        if self.use_bias:\n",
        "            output += self.b\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        if self.W_skip is not None:\n",
        "            skip = tf.matmul(X, self.W_skip)\n",
        "        else:\n",
        "            skip = X\n",
        "\n",
        "        output = output + skip\n",
        "\n",
        "        output = self.dropout(output, training=training)\n",
        "\n",
        "        return self.activation(output) if self.activation else output\n",
        "\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        F = input_shape[-1]\n",
        "        self.W_att = self.add_weight(\n",
        "            name=\"W_attention\",\n",
        "            shape=(F, 1),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        att_scores = tf.matmul(inputs, self.W_att)  # (N, 1)\n",
        "        att_weights = tf.nn.sigmoid(att_scores)     # (N, 1)\n",
        "\n",
        "        return inputs * att_weights\n",
        "\n",
        "class PPIModel(Model):\n",
        "    def __init__(self, hidden_dims=[512, 256], n_classes=121, dropout=0.2,\n",
        "                 l2_reg=1e-5, norm_type='layer', use_attention=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dropout = Dropout(dropout/2)\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.conv_layers = []\n",
        "        for i, dim in enumerate(hidden_dims):\n",
        "            self.conv_layers.append(\n",
        "                GraphConvLayer(dim, activation=\"relu\", dropout_rate=dropout,\n",
        "                              norm_type=norm_type, name=f\"gcn_{i}\")\n",
        "            )\n",
        "\n",
        "        if use_attention:\n",
        "            self.attention = AttentionLayer()\n",
        "\n",
        "        self.classifier = Dense(\n",
        "            n_classes,\n",
        "            activation=\"sigmoid\",\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        X, A = inputs\n",
        "\n",
        "        h = self.input_dropout(X, training=training)\n",
        "\n",
        "        for conv in self.conv_layers:\n",
        "            h = conv([h, A], training=training)\n",
        "\n",
        "        if self.use_attention:\n",
        "            h = self.attention(h)\n",
        "\n",
        "        return self.classifier(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW9ZnUES03yj"
      },
      "outputs": [],
      "source": [
        "model_params = {\n",
        "    'hidden_dims': [512, 256],\n",
        "    'n_classes': n_classes,\n",
        "    'dropout': 0.2,\n",
        "    'l2_reg': 1e-5,\n",
        "    'norm_type': 'layer',\n",
        "    'use_attention': False\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMRsg2Dy06rz"
      },
      "outputs": [],
      "source": [
        "model = PPIModel(**model_params)\n",
        "optimizer = Adam(learning_rate=5e-3)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J0RuZU2q08lu"
      },
      "outputs": [],
      "source": [
        "def evaluate(X, A, Y, mask):\n",
        "    preds = model([X, A], training=False)\n",
        "    preds = tf.cast(preds >= 0.5, tf.int32).numpy()\n",
        "    return f1_score(Y[mask], preds[mask], average=\"micro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Umi-1JYJ0_VR"
      },
      "outputs": [],
      "source": [
        "X_tf = tf.constant(X, dtype=tf.float32)\n",
        "Y_tf = tf.constant(Y, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oPmS9EAl1CV-"
      },
      "outputs": [],
      "source": [
        "_ = model([X_tf, A_sp], training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv7CeJsF1E0q"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(X, A, Y, mask):\n",
        "    with tf.GradientTape() as tape:\n",
        "        preds = model([X, A], training=True)\n",
        "        loss = loss_fn(\n",
        "            Y,\n",
        "            preds,\n",
        "            sample_weight=tf.cast(mask, tf.float32)\n",
        "        )\n",
        "        loss += sum(model.losses)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Gradient clipping\n",
        "    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
        "\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4u-g4Ob1H0S"
      },
      "outputs": [],
      "source": [
        "max_epochs = 300\n",
        "patience = 20\n",
        "checkpoint_path = \"best_ppi_gnn.weights.h5\"\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 50:\n",
        "        return lr\n",
        "    elif epoch < 100:\n",
        "        return lr * 0.5\n",
        "    else:\n",
        "        return lr * 0.1\n",
        "\n",
        "best_val = 0.0\n",
        "wait = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stjcqE9C1MUR",
        "outputId": "efa453d6-677b-40d0-a48f-c8913f6301ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001: loss=0.7194 val_f1=0.4267 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4267)\n",
            "Epoch 002: loss=0.5619 val_f1=0.4175 lr=0.005000\n",
            "Epoch 003: loss=0.4900 val_f1=0.4153 lr=0.005000\n",
            "Epoch 004: loss=0.4659 val_f1=0.4089 lr=0.005000\n",
            "Epoch 005: loss=0.4606 val_f1=0.4181 lr=0.005000\n",
            "Epoch 006: loss=0.4563 val_f1=0.4208 lr=0.005000\n",
            "Epoch 007: loss=0.4506 val_f1=0.4169 lr=0.005000\n",
            "Epoch 008: loss=0.4462 val_f1=0.4147 lr=0.005000\n",
            "Epoch 009: loss=0.4426 val_f1=0.4178 lr=0.005000\n",
            "Epoch 010: loss=0.4396 val_f1=0.4254 lr=0.005000\n",
            "Epoch 011: loss=0.4372 val_f1=0.4308 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4308)\n",
            "Epoch 012: loss=0.4349 val_f1=0.4348 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4348)\n",
            "Epoch 013: loss=0.4330 val_f1=0.4382 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4382)\n",
            "Epoch 014: loss=0.4312 val_f1=0.4409 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4409)\n",
            "Epoch 015: loss=0.4295 val_f1=0.4439 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4439)\n",
            "Epoch 016: loss=0.4275 val_f1=0.4472 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4472)\n",
            "Epoch 017: loss=0.4256 val_f1=0.4487 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4487)\n",
            "Epoch 018: loss=0.4238 val_f1=0.4502 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4502)\n",
            "Epoch 019: loss=0.4222 val_f1=0.4536 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4536)\n",
            "Epoch 020: loss=0.4208 val_f1=0.4573 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4573)\n",
            "Epoch 021: loss=0.4188 val_f1=0.4601 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4601)\n",
            "Epoch 022: loss=0.4175 val_f1=0.4615 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4615)\n",
            "Epoch 023: loss=0.4157 val_f1=0.4619 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4619)\n",
            "Epoch 024: loss=0.4140 val_f1=0.4654 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4654)\n",
            "Epoch 025: loss=0.4123 val_f1=0.4718 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4718)\n",
            "Epoch 026: loss=0.4107 val_f1=0.4799 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4799)\n",
            "Epoch 027: loss=0.4090 val_f1=0.4863 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4863)\n",
            "Epoch 028: loss=0.4072 val_f1=0.4914 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4914)\n",
            "Epoch 029: loss=0.4051 val_f1=0.4957 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4957)\n",
            "Epoch 030: loss=0.4036 val_f1=0.4995 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.4995)\n",
            "Epoch 031: loss=0.4016 val_f1=0.5040 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5040)\n",
            "Epoch 032: loss=0.3998 val_f1=0.5098 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5098)\n",
            "Epoch 033: loss=0.3982 val_f1=0.5161 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5161)\n",
            "Epoch 034: loss=0.3963 val_f1=0.5243 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5243)\n",
            "Epoch 035: loss=0.3944 val_f1=0.5313 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5313)\n",
            "Epoch 036: loss=0.3929 val_f1=0.5399 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5399)\n",
            "Epoch 037: loss=0.3911 val_f1=0.5432 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5432)\n",
            "Epoch 038: loss=0.3891 val_f1=0.5542 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5542)\n",
            "Epoch 039: loss=0.3869 val_f1=0.5543 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5543)\n",
            "Epoch 040: loss=0.3857 val_f1=0.5652 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5652)\n",
            "Epoch 041: loss=0.3838 val_f1=0.5651 lr=0.005000\n",
            "Epoch 042: loss=0.3824 val_f1=0.5728 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5728)\n",
            "Epoch 043: loss=0.3802 val_f1=0.5774 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5774)\n",
            "Epoch 044: loss=0.3788 val_f1=0.5768 lr=0.005000\n",
            "Epoch 045: loss=0.3767 val_f1=0.5887 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5887)\n",
            "Epoch 046: loss=0.3754 val_f1=0.5874 lr=0.005000\n",
            "Epoch 047: loss=0.3738 val_f1=0.5890 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5890)\n",
            "Epoch 048: loss=0.3726 val_f1=0.5998 lr=0.005000\n",
            "Model improved, saving checkpoint (val_f1=0.5998)\n",
            "Epoch 049: loss=0.3714 val_f1=0.5890 lr=0.005000\n",
            "Learning rate decreased to 0.0024999999441206455\n",
            "Epoch 050: loss=0.3703 val_f1=0.6023 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6023)\n",
            "Epoch 051: loss=0.3689 val_f1=0.6095 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6095)\n",
            "Epoch 052: loss=0.3682 val_f1=0.6021 lr=0.002500\n",
            "Epoch 053: loss=0.3674 val_f1=0.5983 lr=0.002500\n",
            "Epoch 054: loss=0.3671 val_f1=0.6094 lr=0.002500\n",
            "Epoch 055: loss=0.3653 val_f1=0.6139 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6139)\n",
            "Epoch 056: loss=0.3650 val_f1=0.6092 lr=0.002500\n",
            "Epoch 057: loss=0.3641 val_f1=0.6066 lr=0.002500\n",
            "Epoch 058: loss=0.3639 val_f1=0.6137 lr=0.002500\n",
            "Epoch 059: loss=0.3630 val_f1=0.6187 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6187)\n",
            "Epoch 060: loss=0.3622 val_f1=0.6148 lr=0.002500\n",
            "Epoch 061: loss=0.3614 val_f1=0.6142 lr=0.002500\n",
            "Epoch 062: loss=0.3610 val_f1=0.6215 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6215)\n",
            "Epoch 063: loss=0.3604 val_f1=0.6248 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6248)\n",
            "Epoch 064: loss=0.3591 val_f1=0.6217 lr=0.002500\n",
            "Epoch 065: loss=0.3590 val_f1=0.6214 lr=0.002500\n",
            "Epoch 066: loss=0.3582 val_f1=0.6274 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6274)\n",
            "Epoch 067: loss=0.3573 val_f1=0.6281 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6281)\n",
            "Epoch 068: loss=0.3570 val_f1=0.6255 lr=0.002500\n",
            "Epoch 069: loss=0.3565 val_f1=0.6288 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6288)\n",
            "Epoch 070: loss=0.3553 val_f1=0.6333 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6333)\n",
            "Epoch 071: loss=0.3556 val_f1=0.6323 lr=0.002500\n",
            "Epoch 072: loss=0.3540 val_f1=0.6316 lr=0.002500\n",
            "Epoch 073: loss=0.3529 val_f1=0.6353 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6353)\n",
            "Epoch 074: loss=0.3530 val_f1=0.6371 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6371)\n",
            "Epoch 075: loss=0.3527 val_f1=0.6359 lr=0.002500\n",
            "Epoch 076: loss=0.3525 val_f1=0.6366 lr=0.002500\n",
            "Epoch 077: loss=0.3516 val_f1=0.6422 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6422)\n",
            "Epoch 078: loss=0.3510 val_f1=0.6413 lr=0.002500\n",
            "Epoch 079: loss=0.3507 val_f1=0.6420 lr=0.002500\n",
            "Epoch 080: loss=0.3491 val_f1=0.6433 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6433)\n",
            "Epoch 081: loss=0.3493 val_f1=0.6443 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6443)\n",
            "Epoch 082: loss=0.3495 val_f1=0.6465 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6465)\n",
            "Epoch 083: loss=0.3478 val_f1=0.6443 lr=0.002500\n",
            "Epoch 084: loss=0.3476 val_f1=0.6478 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6478)\n",
            "Epoch 085: loss=0.3469 val_f1=0.6502 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6502)\n",
            "Epoch 086: loss=0.3462 val_f1=0.6487 lr=0.002500\n",
            "Epoch 087: loss=0.3461 val_f1=0.6516 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6516)\n",
            "Epoch 088: loss=0.3460 val_f1=0.6524 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6524)\n",
            "Epoch 089: loss=0.3451 val_f1=0.6530 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6530)\n",
            "Epoch 090: loss=0.3447 val_f1=0.6536 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6536)\n",
            "Epoch 091: loss=0.3441 val_f1=0.6561 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6561)\n",
            "Epoch 092: loss=0.3426 val_f1=0.6542 lr=0.002500\n",
            "Epoch 093: loss=0.3424 val_f1=0.6555 lr=0.002500\n",
            "Epoch 094: loss=0.3424 val_f1=0.6606 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6606)\n",
            "Epoch 095: loss=0.3424 val_f1=0.6579 lr=0.002500\n",
            "Epoch 096: loss=0.3408 val_f1=0.6565 lr=0.002500\n",
            "Epoch 097: loss=0.3409 val_f1=0.6608 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6608)\n",
            "Epoch 098: loss=0.3407 val_f1=0.6643 lr=0.002500\n",
            "Model improved, saving checkpoint (val_f1=0.6643)\n",
            "Epoch 099: loss=0.3400 val_f1=0.6604 lr=0.002500\n",
            "Learning rate decreased to 0.0012499999720603228\n",
            "Epoch 100: loss=0.3397 val_f1=0.6614 lr=0.001250\n",
            "Epoch 101: loss=0.3391 val_f1=0.6642 lr=0.001250\n",
            "Epoch 102: loss=0.3390 val_f1=0.6652 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6652)\n",
            "Epoch 103: loss=0.3391 val_f1=0.6643 lr=0.001250\n",
            "Epoch 104: loss=0.3383 val_f1=0.6627 lr=0.001250\n",
            "Epoch 105: loss=0.3386 val_f1=0.6641 lr=0.001250\n",
            "Epoch 106: loss=0.3386 val_f1=0.6671 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6671)\n",
            "Epoch 107: loss=0.3374 val_f1=0.6678 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6678)\n",
            "Epoch 108: loss=0.3379 val_f1=0.6665 lr=0.001250\n",
            "Epoch 109: loss=0.3373 val_f1=0.6654 lr=0.001250\n",
            "Epoch 110: loss=0.3369 val_f1=0.6665 lr=0.001250\n",
            "Epoch 111: loss=0.3370 val_f1=0.6689 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6689)\n",
            "Epoch 112: loss=0.3362 val_f1=0.6696 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6696)\n",
            "Epoch 113: loss=0.3363 val_f1=0.6697 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6697)\n",
            "Epoch 114: loss=0.3364 val_f1=0.6704 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6704)\n",
            "Epoch 115: loss=0.3362 val_f1=0.6709 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6709)\n",
            "Epoch 116: loss=0.3363 val_f1=0.6701 lr=0.001250\n",
            "Epoch 117: loss=0.3355 val_f1=0.6688 lr=0.001250\n",
            "Epoch 118: loss=0.3359 val_f1=0.6696 lr=0.001250\n",
            "Epoch 119: loss=0.3352 val_f1=0.6723 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6723)\n",
            "Epoch 120: loss=0.3348 val_f1=0.6729 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6729)\n",
            "Epoch 121: loss=0.3348 val_f1=0.6724 lr=0.001250\n",
            "Epoch 122: loss=0.3347 val_f1=0.6714 lr=0.001250\n",
            "Epoch 123: loss=0.3346 val_f1=0.6724 lr=0.001250\n",
            "Epoch 124: loss=0.3343 val_f1=0.6753 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6753)\n",
            "Epoch 125: loss=0.3344 val_f1=0.6760 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6760)\n",
            "Epoch 126: loss=0.3331 val_f1=0.6745 lr=0.001250\n",
            "Epoch 127: loss=0.3333 val_f1=0.6739 lr=0.001250\n",
            "Epoch 128: loss=0.3336 val_f1=0.6760 lr=0.001250\n",
            "Epoch 129: loss=0.3330 val_f1=0.6772 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6772)\n",
            "Epoch 130: loss=0.3334 val_f1=0.6760 lr=0.001250\n",
            "Epoch 131: loss=0.3325 val_f1=0.6756 lr=0.001250\n",
            "Epoch 132: loss=0.3322 val_f1=0.6771 lr=0.001250\n",
            "Epoch 133: loss=0.3326 val_f1=0.6787 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6787)\n",
            "Epoch 134: loss=0.3315 val_f1=0.6785 lr=0.001250\n",
            "Epoch 135: loss=0.3313 val_f1=0.6773 lr=0.001250\n",
            "Epoch 136: loss=0.3309 val_f1=0.6781 lr=0.001250\n",
            "Epoch 137: loss=0.3308 val_f1=0.6801 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6801)\n",
            "Epoch 138: loss=0.3319 val_f1=0.6811 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6811)\n",
            "Epoch 139: loss=0.3302 val_f1=0.6813 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6813)\n",
            "Epoch 140: loss=0.3295 val_f1=0.6796 lr=0.001250\n",
            "Epoch 141: loss=0.3310 val_f1=0.6805 lr=0.001250\n",
            "Epoch 142: loss=0.3302 val_f1=0.6820 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6820)\n",
            "Epoch 143: loss=0.3302 val_f1=0.6826 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6826)\n",
            "Epoch 144: loss=0.3301 val_f1=0.6815 lr=0.001250\n",
            "Epoch 145: loss=0.3297 val_f1=0.6820 lr=0.001250\n",
            "Epoch 146: loss=0.3285 val_f1=0.6834 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6834)\n",
            "Epoch 147: loss=0.3291 val_f1=0.6847 lr=0.001250\n",
            "Model improved, saving checkpoint (val_f1=0.6847)\n",
            "Epoch 148: loss=0.3286 val_f1=0.6841 lr=0.001250\n",
            "Epoch 149: loss=0.3280 val_f1=0.6827 lr=0.001250\n",
            "Learning rate decreased to 0.0006249999860301614\n",
            "Epoch 150: loss=0.3289 val_f1=0.6832 lr=0.000625\n",
            "Epoch 151: loss=0.3284 val_f1=0.6839 lr=0.000625\n",
            "Epoch 152: loss=0.3282 val_f1=0.6850 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6850)\n",
            "Epoch 153: loss=0.3281 val_f1=0.6856 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6856)\n",
            "Epoch 154: loss=0.3277 val_f1=0.6854 lr=0.000625\n",
            "Epoch 155: loss=0.3272 val_f1=0.6853 lr=0.000625\n",
            "Epoch 156: loss=0.3273 val_f1=0.6857 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6857)\n",
            "Epoch 157: loss=0.3281 val_f1=0.6867 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6867)\n",
            "Epoch 158: loss=0.3279 val_f1=0.6876 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6876)\n",
            "Epoch 159: loss=0.3284 val_f1=0.6873 lr=0.000625\n",
            "Epoch 160: loss=0.3276 val_f1=0.6866 lr=0.000625\n",
            "Epoch 161: loss=0.3272 val_f1=0.6859 lr=0.000625\n",
            "Epoch 162: loss=0.3269 val_f1=0.6860 lr=0.000625\n",
            "Epoch 163: loss=0.3273 val_f1=0.6867 lr=0.000625\n",
            "Epoch 164: loss=0.3266 val_f1=0.6877 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6877)\n",
            "Epoch 165: loss=0.3272 val_f1=0.6887 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6887)\n",
            "Epoch 166: loss=0.3265 val_f1=0.6887 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6887)\n",
            "Epoch 167: loss=0.3263 val_f1=0.6885 lr=0.000625\n",
            "Epoch 168: loss=0.3263 val_f1=0.6872 lr=0.000625\n",
            "Epoch 169: loss=0.3259 val_f1=0.6868 lr=0.000625\n",
            "Epoch 170: loss=0.3263 val_f1=0.6875 lr=0.000625\n",
            "Epoch 171: loss=0.3266 val_f1=0.6890 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6890)\n",
            "Epoch 172: loss=0.3264 val_f1=0.6901 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6901)\n",
            "Epoch 173: loss=0.3264 val_f1=0.6903 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6903)\n",
            "Epoch 174: loss=0.3268 val_f1=0.6896 lr=0.000625\n",
            "Epoch 175: loss=0.3255 val_f1=0.6890 lr=0.000625\n",
            "Epoch 176: loss=0.3253 val_f1=0.6893 lr=0.000625\n",
            "Epoch 177: loss=0.3254 val_f1=0.6905 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6905)\n",
            "Epoch 178: loss=0.3263 val_f1=0.6914 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6914)\n",
            "Epoch 179: loss=0.3251 val_f1=0.6913 lr=0.000625\n",
            "Epoch 180: loss=0.3260 val_f1=0.6911 lr=0.000625\n",
            "Epoch 181: loss=0.3253 val_f1=0.6907 lr=0.000625\n",
            "Epoch 182: loss=0.3258 val_f1=0.6907 lr=0.000625\n",
            "Epoch 183: loss=0.3258 val_f1=0.6914 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6914)\n",
            "Epoch 184: loss=0.3252 val_f1=0.6923 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6923)\n",
            "Epoch 185: loss=0.3253 val_f1=0.6920 lr=0.000625\n",
            "Epoch 186: loss=0.3254 val_f1=0.6916 lr=0.000625\n",
            "Epoch 187: loss=0.3254 val_f1=0.6914 lr=0.000625\n",
            "Epoch 188: loss=0.3238 val_f1=0.6916 lr=0.000625\n",
            "Epoch 189: loss=0.3240 val_f1=0.6925 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6925)\n",
            "Epoch 190: loss=0.3239 val_f1=0.6933 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6933)\n",
            "Epoch 191: loss=0.3241 val_f1=0.6940 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6940)\n",
            "Epoch 192: loss=0.3241 val_f1=0.6936 lr=0.000625\n",
            "Epoch 193: loss=0.3237 val_f1=0.6931 lr=0.000625\n",
            "Epoch 194: loss=0.3230 val_f1=0.6930 lr=0.000625\n",
            "Epoch 195: loss=0.3236 val_f1=0.6939 lr=0.000625\n",
            "Epoch 196: loss=0.3232 val_f1=0.6945 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6945)\n",
            "Epoch 197: loss=0.3232 val_f1=0.6947 lr=0.000625\n",
            "Model improved, saving checkpoint (val_f1=0.6947)\n",
            "Epoch 198: loss=0.3235 val_f1=0.6942 lr=0.000625\n",
            "Epoch 199: loss=0.3220 val_f1=0.6939 lr=0.000625\n",
            "Learning rate decreased to 0.0003124999930150807\n",
            "Epoch 200: loss=0.3227 val_f1=0.6940 lr=0.000312\n",
            "Epoch 201: loss=0.3231 val_f1=0.6945 lr=0.000312\n",
            "Epoch 202: loss=0.3230 val_f1=0.6954 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6954)\n",
            "Epoch 203: loss=0.3237 val_f1=0.6960 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6960)\n",
            "Epoch 204: loss=0.3238 val_f1=0.6965 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6965)\n",
            "Epoch 205: loss=0.3228 val_f1=0.6964 lr=0.000312\n",
            "Epoch 206: loss=0.3234 val_f1=0.6959 lr=0.000312\n",
            "Epoch 207: loss=0.3224 val_f1=0.6954 lr=0.000312\n",
            "Epoch 208: loss=0.3237 val_f1=0.6950 lr=0.000312\n",
            "Epoch 209: loss=0.3229 val_f1=0.6951 lr=0.000312\n",
            "Epoch 210: loss=0.3221 val_f1=0.6954 lr=0.000312\n",
            "Epoch 211: loss=0.3231 val_f1=0.6958 lr=0.000312\n",
            "Epoch 212: loss=0.3227 val_f1=0.6962 lr=0.000312\n",
            "Epoch 213: loss=0.3230 val_f1=0.6966 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6966)\n",
            "Epoch 214: loss=0.3228 val_f1=0.6967 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6967)\n",
            "Epoch 215: loss=0.3229 val_f1=0.6965 lr=0.000312\n",
            "Epoch 216: loss=0.3217 val_f1=0.6961 lr=0.000312\n",
            "Epoch 217: loss=0.3228 val_f1=0.6961 lr=0.000312\n",
            "Epoch 218: loss=0.3223 val_f1=0.6962 lr=0.000312\n",
            "Epoch 219: loss=0.3224 val_f1=0.6965 lr=0.000312\n",
            "Epoch 220: loss=0.3216 val_f1=0.6967 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6967)\n",
            "Epoch 221: loss=0.3225 val_f1=0.6971 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6971)\n",
            "Epoch 222: loss=0.3221 val_f1=0.6973 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6973)\n",
            "Epoch 223: loss=0.3220 val_f1=0.6971 lr=0.000312\n",
            "Epoch 224: loss=0.3223 val_f1=0.6970 lr=0.000312\n",
            "Epoch 225: loss=0.3229 val_f1=0.6971 lr=0.000312\n",
            "Epoch 226: loss=0.3218 val_f1=0.6974 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6974)\n",
            "Epoch 227: loss=0.3223 val_f1=0.6978 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6978)\n",
            "Epoch 228: loss=0.3217 val_f1=0.6984 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6984)\n",
            "Epoch 229: loss=0.3221 val_f1=0.6986 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6986)\n",
            "Epoch 230: loss=0.3223 val_f1=0.6986 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6986)\n",
            "Epoch 231: loss=0.3213 val_f1=0.6984 lr=0.000312\n",
            "Epoch 232: loss=0.3213 val_f1=0.6980 lr=0.000312\n",
            "Epoch 233: loss=0.3218 val_f1=0.6978 lr=0.000312\n",
            "Epoch 234: loss=0.3215 val_f1=0.6979 lr=0.000312\n",
            "Epoch 235: loss=0.3216 val_f1=0.6981 lr=0.000312\n",
            "Epoch 236: loss=0.3211 val_f1=0.6985 lr=0.000312\n",
            "Epoch 237: loss=0.3215 val_f1=0.6989 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6989)\n",
            "Epoch 238: loss=0.3209 val_f1=0.6989 lr=0.000312\n",
            "Epoch 239: loss=0.3212 val_f1=0.6987 lr=0.000312\n",
            "Epoch 240: loss=0.3211 val_f1=0.6986 lr=0.000312\n",
            "Epoch 241: loss=0.3207 val_f1=0.6986 lr=0.000312\n",
            "Epoch 242: loss=0.3213 val_f1=0.6989 lr=0.000312\n",
            "Epoch 243: loss=0.3208 val_f1=0.6995 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6995)\n",
            "Epoch 244: loss=0.3203 val_f1=0.6999 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.6999)\n",
            "Epoch 245: loss=0.3209 val_f1=0.7002 lr=0.000312\n",
            "Model improved, saving checkpoint (val_f1=0.7002)\n",
            "Epoch 246: loss=0.3203 val_f1=0.7001 lr=0.000312\n",
            "Epoch 247: loss=0.3212 val_f1=0.6995 lr=0.000312\n",
            "Epoch 248: loss=0.3202 val_f1=0.6989 lr=0.000312\n",
            "Epoch 249: loss=0.3204 val_f1=0.6986 lr=0.000312\n",
            "Learning rate decreased to 0.00015624999650754035\n",
            "Epoch 250: loss=0.3209 val_f1=0.6987 lr=0.000156\n",
            "Epoch 251: loss=0.3202 val_f1=0.6989 lr=0.000156\n",
            "Epoch 252: loss=0.3211 val_f1=0.6992 lr=0.000156\n",
            "Epoch 253: loss=0.3199 val_f1=0.6995 lr=0.000156\n",
            "Epoch 254: loss=0.3207 val_f1=0.6999 lr=0.000156\n",
            "Epoch 255: loss=0.3199 val_f1=0.7002 lr=0.000156\n",
            "Epoch 256: loss=0.3211 val_f1=0.7005 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7005)\n",
            "Epoch 257: loss=0.3202 val_f1=0.7007 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7007)\n",
            "Epoch 258: loss=0.3215 val_f1=0.7008 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7008)\n",
            "Epoch 259: loss=0.3207 val_f1=0.7008 lr=0.000156\n",
            "Epoch 260: loss=0.3208 val_f1=0.7007 lr=0.000156\n",
            "Epoch 261: loss=0.3201 val_f1=0.7006 lr=0.000156\n",
            "Epoch 262: loss=0.3197 val_f1=0.7002 lr=0.000156\n",
            "Epoch 263: loss=0.3203 val_f1=0.6999 lr=0.000156\n",
            "Epoch 264: loss=0.3198 val_f1=0.6997 lr=0.000156\n",
            "Epoch 265: loss=0.3208 val_f1=0.6998 lr=0.000156\n",
            "Epoch 266: loss=0.3206 val_f1=0.7000 lr=0.000156\n",
            "Epoch 267: loss=0.3194 val_f1=0.7003 lr=0.000156\n",
            "Epoch 268: loss=0.3209 val_f1=0.7007 lr=0.000156\n",
            "Epoch 269: loss=0.3200 val_f1=0.7011 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7011)\n",
            "Epoch 270: loss=0.3210 val_f1=0.7014 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7014)\n",
            "Epoch 271: loss=0.3199 val_f1=0.7014 lr=0.000156\n",
            "Epoch 272: loss=0.3202 val_f1=0.7012 lr=0.000156\n",
            "Epoch 273: loss=0.3202 val_f1=0.7011 lr=0.000156\n",
            "Epoch 274: loss=0.3197 val_f1=0.7009 lr=0.000156\n",
            "Epoch 275: loss=0.3210 val_f1=0.7006 lr=0.000156\n",
            "Epoch 276: loss=0.3197 val_f1=0.7006 lr=0.000156\n",
            "Epoch 277: loss=0.3197 val_f1=0.7005 lr=0.000156\n",
            "Epoch 278: loss=0.3194 val_f1=0.7006 lr=0.000156\n",
            "Epoch 279: loss=0.3201 val_f1=0.7010 lr=0.000156\n",
            "Epoch 280: loss=0.3206 val_f1=0.7013 lr=0.000156\n",
            "Epoch 281: loss=0.3188 val_f1=0.7014 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7014)\n",
            "Epoch 282: loss=0.3205 val_f1=0.7015 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7015)\n",
            "Epoch 283: loss=0.3198 val_f1=0.7016 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7016)\n",
            "Epoch 284: loss=0.3193 val_f1=0.7017 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7017)\n",
            "Epoch 285: loss=0.3198 val_f1=0.7017 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7017)\n",
            "Epoch 286: loss=0.3203 val_f1=0.7017 lr=0.000156\n",
            "Epoch 287: loss=0.3199 val_f1=0.7015 lr=0.000156\n",
            "Epoch 288: loss=0.3211 val_f1=0.7014 lr=0.000156\n",
            "Epoch 289: loss=0.3195 val_f1=0.7014 lr=0.000156\n",
            "Epoch 290: loss=0.3193 val_f1=0.7014 lr=0.000156\n",
            "Epoch 291: loss=0.3199 val_f1=0.7015 lr=0.000156\n",
            "Epoch 292: loss=0.3189 val_f1=0.7016 lr=0.000156\n",
            "Epoch 293: loss=0.3201 val_f1=0.7020 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7020)\n",
            "Epoch 294: loss=0.3201 val_f1=0.7022 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7022)\n",
            "Epoch 295: loss=0.3198 val_f1=0.7024 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7024)\n",
            "Epoch 296: loss=0.3198 val_f1=0.7024 lr=0.000156\n",
            "Model improved, saving checkpoint (val_f1=0.7024)\n",
            "Epoch 297: loss=0.3189 val_f1=0.7024 lr=0.000156\n",
            "Epoch 298: loss=0.3184 val_f1=0.7023 lr=0.000156\n",
            "Epoch 299: loss=0.3187 val_f1=0.7022 lr=0.000156\n",
            "Learning rate decreased to 7.812499825377017e-05\n",
            "Epoch 300: loss=0.3198 val_f1=0.7022 lr=0.000078\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, max_epochs + 1):\n",
        "    if epoch % 50 == 0 and epoch > 0:\n",
        "        lr = optimizer.learning_rate.numpy()\n",
        "        optimizer.learning_rate.assign(lr * 0.5)\n",
        "        print(f\"Learning rate decreased to {optimizer.learning_rate.numpy()}\")\n",
        "\n",
        "    loss = train_step(X_tf, A_sp, Y_tf, train_mask)\n",
        "\n",
        "    val_f1 = evaluate(X_tf, A_sp, Y, val_mask)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}: loss={loss:.4f} val_f1={val_f1:.4f} lr={optimizer.learning_rate.numpy():.6f}\")\n",
        "\n",
        "    if val_f1 > best_val:\n",
        "        best_val = val_f1\n",
        "        wait = 0\n",
        "        model.save_weights(checkpoint_path)\n",
        "        print(f\"Model improved, saving checkpoint (val_f1={val_f1:.4f})\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(f\"Early stopping after {epoch} epochs.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RftRI7kf1cL5",
        "outputId": "9fae5ff0-5294-4745-e4ad-45b752e75cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final results - Val Micro-F1: 0.7024, Test Micro-F1: 0.7179\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ppi_model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"ppi_model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gcn_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GraphConvLayer</span>)          │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gcn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GraphConvLayer</span>)          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,912</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">56944</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,097</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gcn_0 (\u001b[38;5;33mGraphConvLayer\u001b[0m)          │ ?                      │        \u001b[38;5;34m52,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gcn_1 (\u001b[38;5;33mGraphConvLayer\u001b[0m)          │ ?                      │       \u001b[38;5;34m262,912\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m56944\u001b[0m, \u001b[38;5;34m121\u001b[0m)           │        \u001b[38;5;34m31,097\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">346,745</span> (1.32 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m346,745\u001b[0m (1.32 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">346,745</span> (1.32 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m346,745\u001b[0m (1.32 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set performance by class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       0.88      0.87      0.88      3518\n",
            "     class_1       0.83      0.34      0.49      1293\n",
            "     class_2       0.84      0.26      0.39      1138\n",
            "     class_3       0.75      0.49      0.59      1302\n",
            "     class_4       0.79      0.24      0.36       721\n",
            "     class_5       0.76      0.54      0.63      1041\n",
            "     class_6       0.75      0.51      0.61      1321\n",
            "     class_7       0.66      0.34      0.45      1916\n",
            "     class_8       0.88      0.18      0.30       878\n",
            "     class_9       0.88      0.72      0.79      1926\n",
            "    class_10       0.91      0.65      0.76      1192\n",
            "    class_11       0.80      0.47      0.59      1245\n",
            "    class_12       0.75      0.95      0.84      3864\n",
            "    class_13       0.71      0.18      0.29       988\n",
            "    class_14       0.82      0.77      0.80       973\n",
            "    class_15       0.74      0.59      0.66      1981\n",
            "    class_16       0.90      0.36      0.51       561\n",
            "    class_17       0.89      0.04      0.08       781\n",
            "    class_18       0.82      0.36      0.50      1441\n",
            "    class_19       0.85      0.75      0.79      2349\n",
            "    class_20       0.66      0.30      0.41      1049\n",
            "    class_21       0.89      0.85      0.87      3388\n",
            "    class_22       0.89      0.86      0.87      1226\n",
            "    class_23       0.74      0.37      0.50      1814\n",
            "    class_24       0.74      0.08      0.15       911\n",
            "    class_25       0.74      0.92      0.82      3460\n",
            "    class_26       0.77      0.94      0.85      3672\n",
            "    class_27       0.69      0.59      0.64      1823\n",
            "    class_28       0.81      0.94      0.87      3861\n",
            "    class_29       0.85      0.80      0.83       926\n",
            "    class_30       0.87      0.89      0.88      3685\n",
            "    class_31       0.97      0.04      0.08       798\n",
            "    class_32       0.90      1.00      0.94      4895\n",
            "    class_33       0.66      0.14      0.23       847\n",
            "    class_34       0.79      0.47      0.59      1130\n",
            "    class_35       0.71      0.51      0.59      1821\n",
            "    class_36       0.68      0.15      0.24      1064\n",
            "    class_37       0.68      0.48      0.56      1108\n",
            "    class_38       0.92      0.83      0.87      2195\n",
            "    class_39       0.92      0.64      0.75      1176\n",
            "    class_40       0.81      0.14      0.24       812\n",
            "    class_41       0.71      0.10      0.18      1115\n",
            "    class_42       0.73      0.37      0.49      1077\n",
            "    class_43       0.72      0.10      0.18      1068\n",
            "    class_44       0.82      0.01      0.02       714\n",
            "    class_45       0.91      0.84      0.87      1493\n",
            "    class_46       0.87      0.72      0.79      2139\n",
            "    class_47       0.86      0.73      0.79      2183\n",
            "    class_48       0.81      0.37      0.50      1295\n",
            "    class_49       0.79      0.51      0.62      1216\n",
            "    class_50       0.74      0.59      0.65      1906\n",
            "    class_51       0.76      0.49      0.60      1556\n",
            "    class_52       0.82      0.72      0.76      2318\n",
            "    class_53       0.92      0.83      0.87      2220\n",
            "    class_54       0.79      0.48      0.60      1314\n",
            "    class_55       0.91      0.87      0.89      1390\n",
            "    class_56       0.69      0.12      0.21       873\n",
            "    class_57       0.80      0.39      0.53       864\n",
            "    class_58       0.85      0.20      0.33       934\n",
            "    class_59       0.89      0.86      0.87      1224\n",
            "    class_60       0.82      0.73      0.77      2421\n",
            "    class_61       0.81      0.05      0.10       869\n",
            "    class_62       0.71      0.15      0.25      1018\n",
            "    class_63       0.91      0.68      0.78      1176\n",
            "    class_64       0.69      0.23      0.35      1088\n",
            "    class_65       0.88      0.03      0.06       693\n",
            "    class_66       0.72      0.70      0.71      2732\n",
            "    class_67       0.71      0.24      0.36      1794\n",
            "    class_68       0.76      0.78      0.77      2654\n",
            "    class_69       0.71      0.37      0.48      1593\n",
            "    class_70       0.83      0.35      0.49      1295\n",
            "    class_71       0.77      0.08      0.15       911\n",
            "    class_72       0.85      0.82      0.84       858\n",
            "    class_73       0.92      0.84      0.87      1552\n",
            "    class_74       0.82      0.77      0.80      1007\n",
            "    class_75       0.82      0.09      0.17      1057\n",
            "    class_76       0.75      0.38      0.51      1810\n",
            "    class_77       0.81      0.24      0.38       749\n",
            "    class_78       0.91      0.85      0.88      1429\n",
            "    class_79       0.89      0.87      0.88      1236\n",
            "    class_80       0.91      0.68      0.78      1202\n",
            "    class_81       0.71      0.69      0.70      2719\n",
            "    class_82       0.91      0.84      0.87      1481\n",
            "    class_83       0.72      0.29      0.41       998\n",
            "    class_84       0.76      0.27      0.40       867\n",
            "    class_85       0.73      0.83      0.78      3187\n",
            "    class_86       0.89      0.33      0.48       501\n",
            "    class_87       0.83      0.76      0.80      1511\n",
            "    class_88       0.89      0.81      0.85      1473\n",
            "    class_89       0.68      0.50      0.58      1053\n",
            "    class_90       0.74      0.27      0.39       900\n",
            "    class_91       0.90      0.63      0.74      1425\n",
            "    class_92       0.88      0.75      0.81      1786\n",
            "    class_93       0.84      0.79      0.81       935\n",
            "    class_94       0.69      0.10      0.17      1070\n",
            "    class_95       0.76      0.35      0.48      1659\n",
            "    class_96       0.76      0.10      0.18       917\n",
            "    class_97       0.86      0.74      0.79      2252\n",
            "    class_98       0.93      0.83      0.87      2340\n",
            "    class_99       0.80      0.41      0.55      1427\n",
            "   class_100       0.72      0.78      0.75      2787\n",
            "   class_101       0.77      0.48      0.59      1348\n",
            "   class_102       0.64      0.03      0.05       900\n",
            "   class_103       0.83      0.75      0.79      1599\n",
            "   class_104       0.85      0.78      0.81       980\n",
            "   class_105       0.67      0.34      0.45      1563\n",
            "   class_106       0.65      0.21      0.32      1579\n",
            "   class_107       0.86      0.23      0.36      1094\n",
            "   class_108       0.76      0.77      0.77      2620\n",
            "   class_109       0.72      0.41      0.52      2005\n",
            "   class_110       0.75      0.65      0.70      1754\n",
            "   class_111       0.84      0.77      0.81       968\n",
            "   class_112       0.92      0.85      0.88      1477\n",
            "   class_113       0.83      0.76      0.79      1542\n",
            "   class_114       0.66      0.47      0.54      1062\n",
            "   class_115       0.93      0.82      0.87      2243\n",
            "   class_116       0.89      1.00      0.94      4870\n",
            "   class_117       0.84      0.96      0.90      4210\n",
            "   class_118       0.85      0.98      0.91      4486\n",
            "   class_119       0.73      0.84      0.78      3072\n",
            "   class_120       0.89      0.86      0.87      1303\n",
            "\n",
            "   micro avg       0.82      0.64      0.72    200096\n",
            "   macro avg       0.80      0.54      0.60    200096\n",
            "weighted avg       0.81      0.64      0.68    200096\n",
            " samples avg       0.76      0.56      0.61    200096\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "val_f1 = evaluate(X_tf, A_sp, Y, val_mask)\n",
        "test_f1 = evaluate(X_tf, A_sp, Y, test_mask)\n",
        "print(f\"Final results - Val Micro-F1: {val_f1:.4f}, Test Micro-F1: {test_f1:.4f}\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "predictions = model([X_tf, A_sp], training=False).numpy()\n",
        "binary_preds = (predictions >= 0.5).astype(np.int32)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class_names = [f\"class_{i}\" for i in range(n_classes)]\n",
        "print(\"\\nTest set performance by class:\")\n",
        "report = classification_report(\n",
        "    Y[test_mask],\n",
        "    binary_preds[test_mask],\n",
        "    target_names=class_names,\n",
        "    zero_division=0\n",
        ")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
